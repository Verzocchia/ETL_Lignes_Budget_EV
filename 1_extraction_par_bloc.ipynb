{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gzip \n",
    "import glob \n",
    "import xmltodict \n",
    "import pandas as pd \n",
    "import timeit \n",
    "import json \n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import MetaData, text, String, Float, Integer, Boolean, ForeignKey, Select, func, Table\n",
    "from sqlalchemy import JSON\n",
    "from sqlalchemy.orm import sessionmaker, DeclarativeBase, Mapped, mapped_column, relationship\n",
    "from typing import Optional, List \n",
    "\n",
    "engine = sqlalchemy.create_engine('postgresql://verzochia:verzochia@localhost:5432/db_v1')\n",
    "conn = engine.connect()\n",
    "metadata = MetaData()\n",
    "\n",
    "LIST_ANNEXES = ['DATA_AMORTISSEMENT_METHODE', 'DATA_APCP', 'DATA_AUTRE_ENGAGEMENT',\n",
    "        'DATA_CHARGE', 'DATA_CONCOURS', 'DATA_CONSOLIDATION', 'DATA_CONTRAT_COUV', \n",
    "        \"DATA_CONTRAT_COUV_REFERENCE\", \"DATA_CREDIT_BAIL\", \"DATA_DETTE\", \"DATA_EMPRUNT\", \n",
    "        \"DATA_ETAB_SERVICE\", \"DATA_FISCALITE\", \"DATA_FOND_AIDES_ECO\", \"DATA_FOND_COMM_HEBERGEMENT\", \n",
    "        \"DATA_FOND_EUROPEEN\", \"DATA_FOND_EUROPEEN_PROGRAMMATION\", \"DATA_FORMATION\", \n",
    "        \"DATA_FORMATION_PRO_JEUNES\", \"DATA_MEMBRESASA\", \"DATA_ORGANISME_ENG\", \n",
    "        \"DATA_ORGANISME_GROUP\", \"DATA_PATRIMOINE\", \"DATA_PERSONNEL\", \"DATA_PERSONNEL_SOLDE\", \n",
    "        \"DATA_PPP\", \"DATA_PRET\", \"DATA_PROVISION\", \"DATA_RECETTE_AFFECTEE\", \n",
    "        \"DATA_SERVICE_FERROVIAIRE_BUD\", \"DATA_SERVICE_FERROVIAIRE_PATRIM\", \n",
    "        \"DATA_SERVICE_FERROVIAIRE_TER\", \"DATA_SIGNATAIRE\", \"DATA_SIGNATURE\", \n",
    "        \"DATA_SOMMAIRE\", \"DATA_TIERS\", \"DATA_TRESORERIE\", \"DATA_VENTILATION\", \"DATA_FLUX_CROISES\"]\n",
    "\n",
    "LISTE_COL_SUP = [\n",
    "    'MtSup_APVote', 'MtSup_Brut', 'MtSup_BudgetHorsRAR', 'MtSup_Comp', \n",
    "    'MtSup_ICNE', 'MtSup_ICNEPrec', 'MtSup_MtOpeCumul','MtSup_MtOpeInfo', \n",
    "    'MtSup_Net', 'MtSup_ProdChaRat', 'MtSup_RARPrec',\n",
    "    'CaracSup_TypOpe', 'CaracSup_Section', 'CaracSup_ChapSpe',\n",
    "    'CaracSup_ProgAutoLib', 'CaracSup_ProgAutoNum',\n",
    "    'CaracSup_VirCredNum', 'CaracSup_CodeRegion']\n",
    "\n",
    "def extraction_id(fichier : str) -> str : \n",
    "  '''Extrait l'id du fichier, renvoie un str. '''\n",
    "\n",
    "  val_id_fichier = fichier.split(\"/\")[-1].split('.')[0]\n",
    "\n",
    "  return val_id_fichier\n",
    "\n",
    "def parse_fichier(chemin : str) -> dict: \n",
    "  '''Ouvre et parse le fichier gzip en dictionnaire'''\n",
    "\n",
    "  with gzip.open(chemin, 'rb') as fichier_ouvert :\n",
    "\n",
    "   fichier_xml_gzip = fichier_ouvert.read()\n",
    "   fichier_xml = fichier_xml_gzip.decode('latin-1')\n",
    "   fichier_dict = xmltodict.parse(fichier_xml)\n",
    "   \n",
    "  return fichier_dict\n",
    "\n",
    "def extraction_annexe(chemin_annexe : dict, dict_metadonnees : dict) -> list : \n",
    " liste_annexe = []\n",
    " for row in chemin_annexe : \n",
    "  liste_par_ligne = {}\n",
    "  for a, b in row.items() : \n",
    "   liste_par_ligne.update({a : b.get('@V')})\n",
    "   liste_par_ligne.update(dict_metadonnees)\n",
    "  liste_annexe.append(liste_par_ligne)\n",
    " return liste_annexe \n",
    "\n",
    "def extraction_donnees(chemin : dict) -> dict  : \n",
    " dict_annexe = {}\n",
    " for a, b in chemin.items() : \n",
    "   dict_annexe.update({a : b.get('@V')})\n",
    " return dict_annexe \n",
    "\n",
    "def extraction_lignes_budget_liste(chemin : dict, dict_id : dict) -> list :\n",
    " liste_budget = []\n",
    " for lignes in chemin : \n",
    "  dict_ligne = {}\n",
    "  dict_ligne.update(dict_id)\n",
    "  for a, b in lignes.items() :\n",
    "     if a not in ['MtSup', 'CaracSup'] : \n",
    "       dict_ligne.update({a : b.get('@V')}) \n",
    " \n",
    "     elif a == 'MtSup' : \n",
    "       dict_ligne.update({a : json.dumps(b)})\n",
    "       type_m = lignes.get('MtSup')\n",
    " \n",
    "       if isinstance(type_m, dict) : \n",
    "        dict_ligne.update({f\"MtSup_{type_m.get('@Code')}\" : type_m.get('@V')})\n",
    " \n",
    "       elif isinstance(type_m, list) :\n",
    "          for j in b : \n",
    "           dict_ligne.update({f\"MtSup_{j.get('@Code')}\" : j.get('@V')})\n",
    " \n",
    "     elif a == 'CaracSup' :   \n",
    "       type_c = lignes.get('CaracSup')\n",
    "       dict_ligne.update({a : json.dumps(b)})\n",
    "\n",
    "       if isinstance(type_c, dict) :\n",
    "        dict_ligne.update({f\"CaracSup_{type_c.get('@Code')}\" : type_c.get('@V')})\n",
    " \n",
    "       elif isinstance(type_c, list) : \n",
    "          for j in b : \n",
    "           dict_ligne.update({f\"CaracSup_{j.get('@Code')}\" : j.get('@V')})\n",
    "\n",
    "  liste_budget.append(dict_ligne)\n",
    " return liste_budget\n",
    "\n",
    "def extraction_budget(fichier_parse : dict , infos_doc_budgetaire : dict)  -> pd.DataFrame : \n",
    " ''' Extrait toutes les données budgetaires, y compris carac et mtsup '''\n",
    " lignes_budget = fichier_parse['DocumentBudgetaire']['Budget']['LigneBudget'] \n",
    "\n",
    " if isinstance(lignes_budget, dict) : \n",
    "  donnees_budget_prep = extraction_donnees(lignes_budget)\n",
    "  donnees_budget_prep.update(infos_doc_budgetaire)\n",
    "  donnees_budget = [donnees_budget_prep]\n",
    "\n",
    " elif isinstance(lignes_budget, list) : \n",
    "  donnees_budget = extraction_lignes_budget_liste(lignes_budget, infos_doc_budgetaire)\n",
    "\n",
    " df_budget = pd.DataFrame(donnees_budget)\n",
    " return df_budget \n",
    "\n",
    "def extraction_document_budgetaire(fichier_parse : dict, dictionnaire_id : dict) -> pd.DataFrame : \n",
    "  ''' Extrait les métadonnées du fichier pour la table document_budgetaire '''\n",
    "  blocbudget = extraction_donnees(fichier_parse['DocumentBudgetaire']['Budget']['BlocBudget'])\n",
    "  entetedocbudg = extraction_donnees(fichier_parse['DocumentBudgetaire']['EnTeteDocBudgetaire'])\n",
    "  entetebudget = extraction_donnees(fichier_parse['DocumentBudgetaire']['Budget']['EnTeteBudget'])\n",
    "  scellement = {'Date' : fichier_parse['DocumentBudgetaire']['Scellement'].get('@date'),\n",
    "                'Md5' : fichier_parse['DocumentBudgetaire']['Scellement'].get('@md5'), \n",
    "                'Sha1' : fichier_parse['DocumentBudgetaire']['Scellement'].get('@sha1')}\n",
    "\n",
    "  liste_fichier = [{**blocbudget, **entetedocbudg, **entetebudget, **scellement, **dictionnaire_id}]\n",
    "  df_doc_budgetaire = pd.DataFrame(liste_fichier)\n",
    "  colonnes_suppr = df_doc_budgetaire.columns.intersection(['NatCEPL', 'Departement'])\n",
    "  df_doc_budgetaire = df_doc_budgetaire.drop(columns=colonnes_suppr)\n",
    "  df_doc_budgetaire = df_doc_budgetaire.rename(columns={'IdColl' : 'Siret' })\n",
    "  df_doc_budgetaire['Siren'] = df_doc_budgetaire['Siret'].str.slice(0,9)\n",
    "  return df_doc_budgetaire\n",
    "\n",
    "def verification_presence_id_table(id_fichier : str, conn, class_bloc_budget) : \n",
    " '''Renvoie le nombre de fois où Id_Fichier est dans la base'''\n",
    " count = Select(func.count(\"*\")).select_from(class_bloc_budget).where(class_bloc_budget.Id_Fichier == id_fichier)\n",
    " requete = conn.execute(count).fetchone()\n",
    " return requete \n",
    "\n",
    "def transcodage_bloc(dataframe_bloc, dictionnaire_transcodage) : \n",
    "  'Exploite le dictionnaire de données déjà sous format dict'\n",
    "  for col in dataframe_bloc.columns : \n",
    "    if col in dictionnaire_transcodage.keys() : \n",
    "      dataframe_bloc[col] = dataframe_bloc[col].replace(dictionnaire_transcodage.get(col))\n",
    "  return dataframe_bloc\n",
    "\n",
    "def nettoyage_colonnes(df_budget, liste_colonnes_propre) :\n",
    "    ''' Remplace les MtSupRARprec etc. par MtSupRARPrec pour que ça rentre en bdd\n",
    "    C'est pas une belle fonction, ça c'est sûr, mais c'est une rustine qui fonctionne'''\n",
    "    dict_replace = {}\n",
    "    for col_budget in df_budget.columns:\n",
    "        col_budget_min = col_budget.lower()\n",
    "\n",
    "        for col_propre in liste_colonnes_propre:\n",
    "            col_propre_min = col_propre.lower()\n",
    "\n",
    "            if col_budget_min == col_propre_min:\n",
    "                dict_replace.update({col_budget : col_propre})\n",
    "\n",
    "    df_budget = df_budget.rename(columns=dict_replace)\n",
    "    return df_budget\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_bdd(chemin_des_xml, dico_transco_csv) :\n",
    "  ''' Fonction principale : \n",
    "  Extrait les données des xml dans le dossier et les envoie dans la table'''\n",
    "  chemin_xml_entree_glob = glob.glob(os.path.join(chemin_des_xml, \"*.gz\"))\n",
    "\n",
    "  temps_verification = 0\n",
    "  temps_ouverture = 0\n",
    "  temps_extraction = 0\n",
    "  temps_extraction_annexe = 0\n",
    "  temps_injection = 0 \n",
    "\n",
    "  #Prep transco\n",
    "  dico_transco_csv = pd.read_csv(dico_transco_csv)\n",
    "  dico_transco = {j['nom_champ']: eval(j['enum']) for i, j in dico_transco_csv.iterrows()}\n",
    "\n",
    "  #Prep verif integrité\n",
    "  engine = sqlalchemy.create_engine('postgresql://verzochia:verzochia@localhost:5432/db_v1')\n",
    "  conn = engine.connect()\n",
    "  metadata = MetaData()\n",
    " \n",
    "  class Base(DeclarativeBase):\n",
    "     pass\n",
    " \n",
    "  class document_budgetaire(Base) :\n",
    "     __table__ = Table('document_budgetaire', Base.metadata, autoload_with = engine)\n",
    "\n",
    "  for fichier in chemin_xml_entree_glob : \n",
    "   id_fichier = extraction_id(fichier)\n",
    "\n",
    "   if id_fichier is None : \n",
    "    pass \n",
    "   else :\n",
    "    debut_verif = timeit.default_timer()\n",
    "    comptage_presence = verification_presence_id_table(id_fichier, \n",
    "                                                      conn, \n",
    "                                                      document_budgetaire) \n",
    "    if comptage_presence[0] > 0 : \n",
    "       #print(f'Fichier {id_fichier} déjà extrait dans la base ! ')\n",
    "       pass \n",
    "    elif comptage_presence[0] == 0 :\n",
    "     temps_verification += timeit.default_timer() - debut_verif\n",
    "     try : \n",
    "      debut_ouverture = timeit.default_timer()\n",
    "      fichier_parse = parse_fichier(fichier)\n",
    "      chemin_exer = fichier_parse['DocumentBudgetaire']['Budget']['BlocBudget']['Exer']\n",
    "      chemin_nomenclature = fichier_parse['DocumentBudgetaire']['Budget']['EnTeteBudget']['Nomenclature']\n",
    "      dict_metadonnees = {'Id_Fichier' : id_fichier, \n",
    "                         'Nomenclature' : chemin_nomenclature.get('@V'),\n",
    "                         'Exer' : chemin_exer.get('@V')}\n",
    "      dict_id = {'Id_Fichier' : id_fichier}\n",
    "      temps_ouverture += timeit.default_timer() - debut_ouverture \n",
    "\n",
    "      #extraction et insertion doc budget (table centrale)\n",
    "      debut_extraction_doc_budg = timeit.default_timer()\n",
    "      df_doc_budget = extraction_document_budgetaire(fichier_parse, dictionnaire_id= dict_id)\n",
    "      df_doc_budget = transcodage_bloc(df_doc_budget, dico_transco)\n",
    "      temps_extraction += timeit.default_timer() - debut_extraction_doc_budg\n",
    "\n",
    "      debut_insertion_doc = timeit.default_timer()\n",
    "      df_doc_budget.to_sql('document_budgetaire', engine ,if_exists = 'append', index = False, method = 'multi')\n",
    "      temps_injection += timeit.default_timer() - debut_insertion_doc\n",
    "\n",
    "      #df_budget\n",
    "      debut_extraction_budget = timeit.default_timer()\n",
    "      df_budget = extraction_budget(fichier_parse, dict_metadonnees)\n",
    "      df_budget = transcodage_bloc(df_budget, dico_transco)\n",
    "      df_budget = nettoyage_colonnes(df_budget, LISTE_COL_SUP)\n",
    "      temps_extraction += timeit.default_timer() - debut_extraction_budget\n",
    "\n",
    "      debut_insertion_budget = timeit.default_timer()\n",
    "      df_budget.to_sql('bloc_budget', engine ,if_exists = 'append', index = False, method = 'multi')\n",
    "      temps_injection += timeit.default_timer() - debut_insertion_budget\n",
    "\n",
    "      #Annexes \n",
    "\n",
    "      chemin_general_annexe = fichier_parse['DocumentBudgetaire']['Budget']['Annexes']\n",
    "      for data_annexe in LIST_ANNEXES : \n",
    "       annexe_maj = data_annexe.split('DATA_')[1]\n",
    "       data_annexe_min = data_annexe.lower()\n",
    "       try :\n",
    "        debut_extract_annexe = timeit.default_timer()\n",
    "        bloc_annexe = chemin_general_annexe[data_annexe][annexe_maj]\n",
    "        df_annexe = pd.DataFrame(extraction_annexe(bloc_annexe, dict_metadonnees))\n",
    "        df_annexe = transcodage_bloc(df_annexe, dico_transco)\n",
    "        temps_extraction_annexe += timeit.default_timer() - debut_extract_annexe\n",
    "\n",
    "        debut_insertion_annexe = timeit.default_timer()\n",
    "        df_annexe.to_sql(data_annexe_min, engine, if_exists = 'append', index = False, method = 'multi')\n",
    "        temps_injection += timeit.default_timer() - debut_insertion_annexe\n",
    "       except Exception as e : \n",
    "         None \n",
    "         \n",
    "     except Exception as e : \n",
    "       print(id_fichier, 'erreur')\n",
    "       print(e)\n",
    "  \n",
    "  print('temps verif :', temps_verification)\n",
    "  print('temps ouverture:', temps_ouverture)\n",
    "  print('temps extract :', temps_extraction)\n",
    "  print('temps injection : ', temps_injection)\n",
    "  print('temps extract annexe :', temps_extraction_annexe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2min 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps verif : 0.013287651352584362\n",
      "temps ouverture: 0.08138989750295877\n",
      "temps extract : 0.07118840236216784\n",
      "temps injection :  0.7903920570388436\n",
      "temps extract annexe : 0.0861179307103157\n"
     ]
    }
   ],
   "source": [
    "xml_to_bdd('../fichiers20/todo_xml_20/test_10/', '../data/explo/dictionnaire_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps verif : 0.927642535418272\n",
      "temps ouverture: 14.482841822318733\n",
      "temps extract : 6.760303259827197\n",
      "temps injection :  80.18578435946256\n",
      "temps extract annexe : 9.188593508675694\n"
     ]
    }
   ],
   "source": [
    "xml_to_bdd('../fichiers20/todo_xml_20/test_1k/', dico_transco_csv= '../data/explo/dictionnaire_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748147 erreur\n",
      "'Siret'\n",
      "775775 erreur\n",
      "no element found: line 1, column 0\n",
      "temps verif : 18.317957990802824\n",
      "temps ouverture: 286.10284290928394\n",
      "temps extract : 132.02825494110584\n",
      "temps injection :  2087.4742519063875\n",
      "temps extract annexe : 170.4289360269904\n"
     ]
    }
   ],
   "source": [
    "xml_to_bdd('../fichiers20/todo_xml_20/', dico_transco_csv= '../data/explo/dictionnaire_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temps verif : 18.317957990802824\n",
    "temps ouverture: 286.10284290928394\n",
    "temps extract : 132.02825494110584\n",
    "temps injection :  2087.4742519063875\n",
    "temps extract annexe : 170.4289360269904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_verif = pd.read_sql('bloc_budget', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TypOpBudg\n",
       "Opération de section à section           51\n",
       "Opération à l'intérieur d'une section    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_verif['TypOpBudg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_document(chemin_des_xml) :\n",
    " ''' Permet d'extraire localement les doc budgetaire'''\n",
    " chemin_xml_entree_glob = glob.glob(os.path.join(chemin_des_xml, \"*.gz\"))\n",
    " #connection à la table\n",
    " liste_df = []\n",
    "\n",
    " for fichier in chemin_xml_entree_glob : \n",
    "  id_fichier = extraction_id(fichier)\n",
    "  #print(id_fichier)\n",
    "  #Necessite verif dans tables\n",
    "  if id_fichier is None : \n",
    "   print('vide')\n",
    "   pass \n",
    "  else : \n",
    "   try : \n",
    "    #print('etape 2')\n",
    "    fichier_parse = parse_fichier(fichier)\n",
    "    dict_id = {'Id_Fichier' : id_fichier}\n",
    "    #print(dict_metadonnees)\n",
    "    df_doc = extraction_document_budgetaire(fichier_parse, dict_id)\n",
    "    liste_df.append(df_doc)\n",
    "    #insertion dans table\n",
    "   except Exception as e : \n",
    "     print(id_fichier, 'erreur')\n",
    "     print(e)\n",
    " \n",
    " df_mega = pd.concat(liste_df)\n",
    " return df_mega "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
